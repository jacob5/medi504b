---
title: "Building a Predictive Machine Learning Model to Identify Polycystic Ovary Syndrome Using Easily Measured Clinical or Physiological Parameters"
author: "Jacob and Hanwei"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r libraries, echo=FALSE, message=FALSE, warning = FALSE, results = "hide"}
# loading libraries

library(tidyverse)
library(ggplot2)
library(scales)
library(finalfit)
library(patchwork)
library(readr)
library(epiR)
library(readxl)
library(gridExtra)
library(janitor)
library(ranger)
library(xgboost)
library(DataExplorer)
library(caret)
library(table1)

# install.packages("table1")

# install.packages("DataExplorer")

# install.packages("janitor")

# install.packages("ranger")

# install.packages("xgboost")


```

## 1. Introduction and Background

### 1.1 Introduction
Our overarching goal for this project is to produce a machine learning (ML) model which can accurately predict polycystic ovary syndrome (PCOS) status (presence or absence of disease) using predictors which may be easily acquired from common clinical settings, for instance, information acquired from a standard blood test and routine clinical examination. PCOS is an endocrine (hormonal) disorder that affects females of a reproductive age^1^. Given the widespread nature of this condition among women of reproductive age and the troubling symptoms which accompany it, including infertility, it would be helpful for physicians to be able to predict, using inexpensive, minimally-invasive and readily available methods, individuals more likely to experience PCOS thereby enabling them to therapeutically intervene, support, advise or provide care in a timely manner, especially considering PCOS has been associated with other conditions such as endometriosis and endometrial cancer^2,5^. 

### 1.2 Background 

Polycystic ovarian syndrome (PCOS) is a common endocrine disorder affecting approximately 10-15% of reproductive-age women worldwide^1^. The condition is characterized by a complex set of symptoms, including hyperandrogenism, menstrual irregularities, and polycystic ovaries^2^. The diagnosis of PCOS is typically based on clinical and biochemical assessments, as well as ultrasound imaging of the ovaries^1^. However, the diagnosis of PCOS can be challenging due to the heterogeneous presentation of symptoms and the lack of a single diagnostic criterion.

Machine learning models have shown promise as a potential tool for the accurate prediction of PCOS. Compared to traditional diagnostic methods, machine learning models can utilize large amounts of data from various sources and provide more accurate predictions. This is particularly beneficial in the case of PCOS, as traditional diagnostic methods such as tissue biopsy or ultrasound imaging can be expensive and invasive. Furthermore, machine learning models can assist clinicians in identifying patients who may benefit from early intervention, which can improve long-term health outcomes^1^.

Our aim for the PCOS predictive model we are building would be to assist clinicians with identifying women between the ages of 21-47 at highest risk or likely to develop PCOS. Since current methods of PCOS diagnosis often require specialized equipment, we sought to develop a model that used a subset of easily measurable clinical and physiological parameters to predict PCOS, thus enabling early diagnoses to be made and enabling specialized resources to be used in fewer patients as a confirmatory test. 

The appropriate ethical approval from Ethics Research Boards was obtained for this ML project. To help ensure privacy fo this ML project, we worked with de-identified anonymized data. In the event an individual was identified as being pregnant or may be at risk of a life-threatening condition, we followed-up immediately with the individual's physician. 

The rationale for developing this predictive ML model is based on the motivation to (1) provide a data-driven method of diagnosis that is cheaper and less invasive than current methods, (2) develop a method which may be applied without expensive diagnostic equipment (such as an ultrasound imaging device), (3) develop a method which may be used with just a blood test and the review of clinical symptoms and (4) construct a method which relies minimally on self-reported data (as this data can be highly variable and sometimes unreliable). As discussed in the background section of this report, current diagnosis of PCOS can be time-consuming and invasive, and replacing these methods with a model is advantageous for reasons related to clinical care, resource utilization and accessibility. This influenced our variable selection, as we did not consider the inclusion of predictors that cannot be measured in the above stated context in model development. We therefore, for example, exclude self-reported variables or variables which required ultrasound imaging (for example, endometrium thickness). 

While accuracy was a key consideration for evaluating model performance, we also focused on optimizing the specificity of the model predictions. The rationale for optimizing specificity was based on the relatively high prevalence of PCOS^7^ as well as the relatively low mortality associated with the disease^1,2,5^. Since patients flagged for high risk of PCOS would receive a confirmatory ultrasound, we wanted to select a model that would be biased towards reducing false positives, as one of our goals was reducing the cost of care. 

Overall, the use of machine learning models to predict PCOS has the potential to improve the accuracy of diagnosis and reduce the cost and invasiveness of traditional diagnostic methods. We sought to evaluate the comparative accuracy of multiple machine learning classifiers to select the optimal model for predicting PCOS based on variables obtained via a blood test and routine clinical examination, given considerations surrounding false positive and false negatives.  

### 1.3 Data

The dataset consists of physical and clinical parameters collected from 10 hospitals across Kerala, India, and from 541 women, to determine PCOS and infertility-related issues. The dataset contains information that can be used to analyze and understand the diagnosis and treatment of PCOS and infertility.

## 2. Objectives

Our objectives were twofold: 

1: Develop a simple model that can predict PCOS status using clinical and physiologic data that can be acquired using a routine blood test and assessment by a general practitioner clinician. 

2: Optimize the model for specificity, thus minimizing the risk of a false positive in model predictions. 

Optimizing model specificity for the prediction of PCOS is important because PCOS is associated with several negative health outcomes, including infertility, insulin resistance, and metabolic disorders. Early diagnosis and treatment of PCOS can help prevent or manage these conditions, which can ultimately improve the overall health and quality of life of those affected. However, given that PCOS is not a life-threatening condition in and of itself, it is important to balance the trade-off between maximizing sensitivity and specificity in order to minimize the number of false positives and prevent unnecessary and potentially invasive follow-up testing. By optimizing model specificity, we can ensure that those who are diagnosed with PCOS are more likely to truly have the condition, while also reducing the risk of unnecessary medical interventions for those who do not have PCOS.

## 3. Methods

### 3.1 Splitting our dataset into training and testing datasets
To ensure data privacy, we worked with de-identified anonymized data. Following our Exploratory Data Analysis (EDA, please refer to Results section for EDA), we started by splitting our dataset into the training set and the testing dataset, followed by building a simple model aimed at using our data to find factors that predict PCOS status (our outcome variable). We chose to partition our data into only two sets based on the relatively small overall sample size of our data and small effective sample size of our data. Further, to help determine generalizability, we believe it would be ideal for our predictive model to be validated on an external dataset derived from a different population than our training and test data. For example, an external validation could be performed on PCOS hospital data from other southern Indian states such as Tamil Nadu or Karnataka. For an even better gauge of model generalizability, the external validation could be performed on PCOS hospital data from other countries.    

### 3.2 Variable selection and building logistic regression models 

We first attempted to model our data with PCOS status as the outcome variable using three different logistic regression models. At this stage of variable selection, we undertook a combined approach to informing variable selection. The Akaike Information Criteria (AIC) was used to assess model fit, the Area Under the Receiving Operator Characteristic (AUC) was used to assess model accuracy, and the model with the best balance between both of these parameters, as well as clinical utility was used. The three tested models utilized the following construction:

* (1) Model 1 is the physiological model which includes five hormones of interest, namely hormone measurements of n=5 different types of hormones commonly included or easily measured/acquired as a part of a standard routine blood test.
* (2) Model 2 is the clinical model, including as variables clinical symptoms associated with PCOS, and specifically clinical symptoms which may be easily determined as a part of a routine examination by a physician. These are the variables included in the clinical model: weight gain, hair growth + skin darkening, hair loss and pimples.
* (3) Model 3 includes both the physiological and clinical variables, that is the n=5 the hormone measurement levels we as well as the clinical symptoms that have been known to be associated with PCOS: weight gain, hair growth + skin darkening, hair loss and pimples (clinical symptoms which may be easily identified or tracked in a basic clinical setting). 

Following this initial variable selection, we selected the two best performing models and applied various machine learning training methods to optimize model performance. We then selected the best performing model as the final model. 

Of our three logistic regression models, Model 2 had the lowest AIC score of 317, followed by Model 3 with an AIC score of 566. However, the predictive performance as assesed by AUC was higher for model 3 (0.99) than model 2 (0.88). While model 3 was better performing, it also required more parameters for prediction that require blood testing. The rationale for testing both models with machine learning optimization was to assess if a simpler model with clinical parameters only could approximate the classification performance of a more complex model. 

## 4. Results

``` {r loading, echo=FALSE, message=FALSE, warning = FALSE, results = "hide"}
# loading data
## setwd("/Users/hanweisudderuddin/Desktop/MSc Stuff/Classes/MEDI504B/Week1_lab_EDA")
setwd(here::here())
# PCOS_infertility <- read_csv("PCOS_infertility.csv")
data_loc <- here::here()
# reading in PCOS_data_without_infertility from excel file
PCOS_data_without_infertility <-read_excel(paste0(data_loc, "/PCOS_data_without_infertility.xlsx"), sheet = "Full_new")
# PCOS_infertility <- read_csv("PCOS_infertility.csv")



# glimpse(PCOS_data_without_infertility)

# view(PCOS_data_without_infertility)

# summary(PCOS_data_without_infertility)

```


### 4a. Exploratory Data Analysis (EDA)
This section contains the steps and output for the EDA performed on the PCOS dataset. The section proceeds sequentially with each step of EDA (please see Appendix for more details). The first steps in our EDA involved getting a high-level overview of our dataset and determining the dimension of our data: n=541 rows and n=45 columns. Further checking of the dataset reveals an additional column (column 45). This column does not contain any useful information and is not one of our 44 features, therefore the column was removed. 


```{r cleaning, echo=FALSE, fig.height=10, fig.width=20, message=FALSE, warning=FALSE, results= "hide"}
## checking and cleaning dataset 
unique(PCOS_data_without_infertility$...45)

## removing column "...45" to clean dataset

PCOS_data_without_infertility <- PCOS_data_without_infertility %>%
                                    select(-...45)

# glimpse(PCOS_data_without_infertility)

# view(PCOS_data_without_infertility)

## cleaning variable names with janitor package

PCOS_data_without_infertility_cleaned <- PCOS_data_without_infertility %>%
                                          clean_names()

# glimpse(PCOS_data_without_infertility_cleaned)

```

### 4a.1 Data Wrangling

The data was formatted to ensure the variables are the appropriate class type, this will enable us to perform our EDA. Specifically we converted binary variables (1 or 0) into the character class type.Next in our EDA we sought to identify missing values in our dataset. Figure A1 in Appendix A shows the missing values in our dataset. Our EDA identified some missing values for two variables: fast food and marriage status. The analysis indicates that only 0.18% of the rows for these variables are missing. Therefore, as this is below the generally used threshold of 5 %, missing values were simply removed for our subsequent analyses. 

```{r formatting, echo=FALSE, fig.height=10, fig.width=20, message=FALSE, warning=FALSE, results="hide"}
## ensuring variables are in the correct format 

PCOS_data_without_infertility_formatted <- PCOS_data_without_infertility_cleaned %>%
                                              mutate_at(c("sl_no",
                                                          "patient_file_no",
                                                          "pcos_y_n",
                                                          "weight_gain_y_n", 
                                                          "hair_growth_y_n",
                                                          "hair_growth_y_n",
                                                          "skin_darkening_y_n",
                                                          "hair_loss_y_n",
                                                          "pimples_y_n",
                                                          "fast_food_y_n",
                                                         
                                                                                                                    "reg_exercise_y_n"), as.character)%>%
  mutate_at(c(
    "i_beta_hcg_m_iu_m_l",
    "fsh_m_iu_m_l" ,
    "lh_m_iu_m_l" ,
    "tsh_m_iu_l" ,
    "amh_ng_m_l", 
    "avg_f_size_l_mm", 
    "avg_f_size_r_mm", 
    "endometrium_mm", 
    "prg_ng_m_l", 
    "vit_d3_ng_m_l",
     "cycle_r_i",
                                                          "cycle_length_days", 
    "prl_ng_m_l"
    
    ), as.numeric)
PCOS_data_without_infertility_formatted$PCOS_Status <- as.factor(ifelse(PCOS_data_without_infertility_formatted$pcos_y_n==1, "Positive", "Negative"))
# glimpse(PCOS_data_without_infertility_formatted)

```


### Table 1: Clinical and sociodemographic characteristics of the study cohort (n=541, women from Kerala, India)

Now that the data is cleaned, we can produce the standard table 1, common in biomedical research for describing the study cohort.

```{r eval=FALSE, include=FALSE}

tab1<-table1::table1(~
age_yrs+              
weight_kg+            
height_cm+           
bmi+                  
blood_group+          
pulse_rate_bpm+      
rr_breaths_min+       
hb_g_dl+              
cycle_r_i+           
cycle_length_days+    
marraige_status_yrs+  
pregnant_y_n+        
no_of_aborptions+     
as.numeric(i_beta_hcg_m_iu_m_l)+  
as.numeric(ii_beta_hcg_m_iu_m_l)+
as.numeric(fsh_m_iu_m_l)+         
as.numeric(lh_m_iu_m_l)+          
fsh_lh+              
hip_inch+             
waist_inch+           
waist_hip_ratio+     
as.numeric(tsh_m_iu_l)+           
as.numeric(amh_ng_m_l)+           
as.numeric(prl_ng_m_l)+          
as.numeric(vit_d3_ng_m_l)+        
as.numeric(prg_ng_m_l)+          
as.numeric(rbs_mg_dl)+           
weight_gain_y_n+      
hair_growth_y_n+      
skin_darkening_y_n+  
hair_loss_y_n+        
pimples_y_n+          
fast_food_y_n+       
reg_exercise_y_n+     
bp_systolic_mm_hg+    
bp_diastolic_mm_hg+  
follicle_no_l+        
follicle_no_r+       
as.numeric(avg_f_size_l_mm)+     
as.numeric(avg_f_size_r_mm)+      
as.numeric(endometrium_mm) | as.factor(PCOS_Status), 
data=PCOS_data_without_infertility_formatted)%>%as.data.frame()
# 
# tab1%>%mutate(`Standardized Difference`=
#                 as.numeric(str_split_fixed(Positive, " ", 2)[,1])/as.numeric(str_split_fixed(Negative, " ", 2)[,1]))
# 
# vec <- as.numeric(str_split_fixed(tab1$Positive, " ", 2)[,1])/
#   as.numeric(str_split_fixed(tab1$Negative, " ", 2)[,1])
# tab1$`Standardized Diff` <- vec
# View(tab1)
```


### Pruned table1

For this table, we have included only the parameters we deemed eligible for the model. Thus, we have excluded self-report variables, expensive and/or invasive tests, and clinical variables not easily measured by a routine clinical examination or a blood test.   

```{r pruned_table, echo=FALSE, message=FALSE, warning=FALSE}
table1::table1(~
age_yrs+              
bmi+                  
pulse_rate_bpm+      
rr_breaths_min+       
hb_g_dl+              
pregnant_y_n+        
as.numeric(i_beta_hcg_m_iu_m_l)+  
as.numeric(fsh_m_iu_m_l)+         
as.numeric(lh_m_iu_m_l)+          
as.numeric(tsh_m_iu_l)+           
as.numeric(amh_ng_m_l)+           
as.numeric(prl_ng_m_l)+          
as.numeric(vit_d3_ng_m_l)+        
as.numeric(prg_ng_m_l)+          
as.numeric(rbs_mg_dl)+           
weight_gain_y_n+      
hair_growth_y_n+      
skin_darkening_y_n+  
hair_loss_y_n+        
pimples_y_n| as.factor(PCOS_Status), 
data=PCOS_data_without_infertility_formatted)
```

From table 1, we can observe some differences among women with and without PCOS in the following parameters: 
Clinical: cycle_r_i, cycle_length_days, weight_gain_y_n, hair_growth_y_n, skin_darkening_y_n, hair_loss_y_n, pimples_y_n, reg_exercise_y_n

Physiologic: i_beta_hcg_m_iu_m_l (Human chorionic gonadotropin), fsh_m_iu_m_l (Follicle Stimulating hormone), lh_m_iu_m_l (Lutenizing hormone), amh_ng_m_l (Anti-mullerian hormone), vit_d3_ng_m_l (Vitamin D), prg_ng_m_l (Progesterone). 

We can visualize these distributions in the following figures. 

### 4a.2 Bivariate associations 

We visualized associations for selected categorical predictors and the outcome (PCOS) by constructing bivariate plots for the predictors stratified by PCOS status^4^. 

```{r EDA4, echo=FALSE, fig.height=10, fig.width=20, message=FALSE, warning=FALSE, results= "hide"}
# plotting bar plots for each categorical feature
barPlotdata <- PCOS_data_without_infertility_formatted %>% select(
  weight_gain_y_n, hair_growth_y_n, skin_darkening_y_n, hair_loss_y_n, 
  hair_loss_y_n, pimples_y_n, PCOS_Status
)

plot_bar(barPlotdata,
         by = "PCOS_Status",
         title = "Barplots for the categorical variables in our dataset")

```

From the figure, we can see that the proportion of individuals with weight gain and a positive PCOS status is greater than the proportion of individuals with no weight gain and a positive PCOS status. We observe similar associations for hair growth and skin darkening. We also observe a similar but much weaker association for the categorical variables of hair loss and pimples. Interestingly, we observe the inverse trend for the categorical variable for fast food consumption. As these predictors appear to be visually associated with the outcome, as well as physiologically feasible to be associated with PCOS, they should be considered for inclusion in the predictive model. 

The bar plots for regular exercise and pregnancy do not appear to show a significant difference in proportion of positive PCOS cases, but we would need to perform a statistical analyses (such as an unpaired t-test) to know for sure. 
To ascertain the relationship between PCOS status and continuous predictors, we also plotted boxplots to visualize any associations between PCOS status and a continuous variable. Please refer to the appendix for these boxplots.

For this section of the EDA, we looked more closely at the relationships between selected variables and our outcome variable of interest (for this project: yes or no for PCOS), based on how our boxplots looked initially (please refer to Appendix for all boxplots) and on some knowledge gained via our literature survey, we will highlight the relatiionship of select variables with our outcome variable (PCOS status). While the etiology for PCOS is not known, our literature survey suggests PCOS is associated with abnormal hormone levels. Thus, to look into this further as part of the EDA, we compared measurements of hormone levels and PCOS status to get a sense of the relationships between these variables.

### Boxplots looking at relationship of selected physiologic parameters and PCOS status (note log-scale)
```{r hormonevariables, echo=FALSE, fig.height=10, fig.width=20, message=FALSE, warning=FALSE, results= "hide"}

## looking more closely and the relationships between selected variables and our outcome variable of interest (PCOS yes or no)

## I   beta-HCG(mIU/mL) hormone 

# glimpse(PCOS_data_without_infertility_formatted)

eda_compare_plot1 <- PCOS_data_without_infertility_formatted %>%
                        ggplot(aes(pcos_y_n, i_beta_hcg_m_iu_m_l)) + 
                        geom_boxplot() +
                        theme_bw() +
                        theme(title = element_text(size = 22),
                              plot.caption = element_text(hjust = 0, size = 20)) +
                        labs(title = "Beta-HCG and PCOS status") + scale_y_log10() 

# eda_compare_plot1


## follicle stimulating hormone

eda_compare_plot2 <- PCOS_data_without_infertility_formatted %>%
                        ggplot(aes(pcos_y_n, fsh_m_iu_m_l)) + 
                        geom_boxplot() +
                        theme_bw() +
                        theme(title = element_text(size = 22),
                              plot.caption = element_text(hjust = 0, size = 20)) +
                        labs(title = "Follicle stimulating hormone and PCOS status") +
                        scale_y_log10()

# eda_compare_plot2


## luteinizing hormone

eda_compare_plot3 <- PCOS_data_without_infertility_formatted %>%
                        ggplot(aes(pcos_y_n, lh_m_iu_m_l)) + 
                        geom_boxplot() +
                        theme_bw() +
                        theme(title = element_text(size = 22),
                              plot.caption = element_text(hjust = 0, size = 20)) +
                        labs(title = "Luteinizing hormone and PCOS status") +
                        scale_y_log10()

# eda_compare_plot4

## Thyroid Stimulating Hormone

eda_compare_plot4 <- PCOS_data_without_infertility_formatted %>%
                        ggplot(aes(pcos_y_n,tsh_m_iu_l)) + 
                        geom_boxplot() +
                        theme_bw() +
                        theme(title = element_text(size = 22),
                              plot.caption = element_text(hjust = 0, size = 20)) +
                        labs(title = "Thyroid Stimulating Hormone and PCOS status") +
                        scale_y_log10()

# eda_compare_plot4
eda_compare_plot5 <- PCOS_data_without_infertility_formatted %>%
                        ggplot(aes(pcos_y_n,vit_d3_ng_m_l)) + 
                        geom_boxplot() +
                        theme_bw() +
                        theme(title = element_text(size = 22),
                              plot.caption = element_text(hjust = 0, size = 20)) +
                        labs(title = "Vitamin D and PCOS status") +
                        scale_y_log10()

eda_compare_plot6 <- PCOS_data_without_infertility_formatted %>%
                        ggplot(aes(pcos_y_n,prg_ng_m_l)) + 
                        geom_boxplot() +
                        theme_bw() +
                        theme(title = element_text(size = 22),
                              plot.caption = element_text(hjust = 0, size = 20)) +
                        labs(title = "Progesterone and PCOS status") +
                        scale_y_log10()


cowplot::plot_grid(eda_compare_plot1, eda_compare_plot2, eda_compare_plot3, eda_compare_plot4, eda_compare_plot5, eda_compare_plot6, ncol=2, nrow=3)
```


To get a sense of the variation in our dataset, we plotted histograms for each continuous variable using the plot_histogram() from the DataExplorer Package. We see that the age distribution in our dataset reveals most individuals are in the range of 20 to 40 years. No individuals in the dataset are younger than 20 or older than 50. As we would expect, the BMI values follow an approximately normal distribution. The most common blood type we observe is O+, which is consistent with the fact that O+ is the most frequent bloodtype globally. The most common cycle length is 5 days. The endometrium thickness data suggests there are two most most common thickness values (two clear peaks in the distribution).

### Modelling

```{r methods, echo=FALSE, message=FALSE, warning = FALSE, results = "hide"}

# loading data
# setwd("/Users/hanweisudderuddin/Desktop/MSc Stuff/Classes/MEDI504B/Week1_lab_EDA")
setwd(here::here())
# PCOS_infertility <- read_csv("PCOS_infertility.csv")
data_loc <- here::here()
# reading in PCOS_data_without_infertility from excel file
PCOS_data_without_infertility <-read_excel(paste0(data_loc, "/PCOS_data_without_infertility.xlsx"), sheet = "Full_new")

## checking and cleaning dataset 
unique(PCOS_data_without_infertility$...45)

## removing column "...45" to clean dataset

PCOS_data_without_infertility <- PCOS_data_without_infertility %>%
                                    select(-...45)

# glimpse(PCOS_data_without_infertility)

# view(PCOS_data_without_infertility)

## cleaning variable names with janitor package

PCOS_data_without_infertility_cleaned <- PCOS_data_without_infertility %>%
                                          clean_names()

glimpse(PCOS_data_without_infertility_cleaned)

PCOS_data_without_infertility_formatted <- PCOS_data_without_infertility_cleaned %>%
                                              mutate_at(c("sl_no",
                                                          "patient_file_no",
                                                          "pcos_y_n",
                                                          "weight_gain_y_n", 
                                                          "hair_growth_y_n",
                                                          "hair_growth_y_n",
                                                          "skin_darkening_y_n",
                                                          "hair_loss_y_n",
                                                          "pimples_y_n",
                                                          "fast_food_y_n",
                                                          "reg_exercise_y_n"), as.factor)%>%
    mutate_at(c(
    "i_beta_hcg_m_iu_m_l",
    "fsh_m_iu_m_l" ,
    "lh_m_iu_m_l" ,
    "tsh_m_iu_l" ,
    "vit_d3_ng_m_l", 
    "amh_ng_m_l"), as.numeric)

glimpse(PCOS_data_without_infertility_formatted)

PCOS_data_without_infertility_formatted_final <- PCOS_data_without_infertility_formatted %>%
                                              drop_na()

unique(PCOS_data_without_infertility_formatted_final$pcos_y_n)

# changing outcome variable to factor class 
PCOS_data_without_infertility_formatted_final$pcos_y_n <- as.factor(PCOS_data_without_infertility_formatted_final$pcos_y_n)
```

```{r, data split, echo=FALSE, message=FALSE, warning = FALSE, results = "hide"}
#####
## setting the seed (enables randomization in a reproducible way)
set.seed(504)

data_split <- caret::createDataPartition(PCOS_data_without_infertility_formatted_final$pcos_y_n, p=0.7, list= FALSE)

train.data <- PCOS_data_without_infertility_formatted_final[data_split,]
# dim(train.data)
test.data <- PCOS_data_without_infertility_formatted_final[-data_split,]
# dim(test.data)

# table(train.data$pcos_y_n)

# the first model we will build will include all the variables 
#library(glmnet)

logistic_model1 <- glm(pcos_y_n ~
                         amh_ng_m_l+                                                                           i_beta_hcg_m_iu_m_l +
                         lh_m_iu_m_l + 
                         fsh_m_iu_m_l + 
                          tsh_m_iu_l+
                         vit_d3_ng_m_l,
                       data = train.data,
                       family = binomial(link="logit")) 


# AIC = 758
# removed this as not super informative for logistic models 
# plot(logistic_model1)

library(broom)

# the second model uses clinical data only

logistic_model2 <-glm(pcos_y_n ~ 
                          weight_gain_y_n +
                         hair_growth_y_n +
                         skin_darkening_y_n +
                         hair_loss_y_n +
                         pimples_y_n,
                      data = train.data,
                      family = binomial(link="logit"))


# AIC = 609.28

# glimpse(PCOS_data_without_infertility_formatted_final)

# the third model we will build will include the following variables:
# 5 of the hormone measurement levels we possess
# as well as the following physiological variables (as these are clinical symptoms associated with PCOS): 

logistic_model3 <- glm(pcos_y_n ~ 
                         i_beta_hcg_m_iu_m_l +
                         fsh_m_iu_m_l + 
                         lh_m_iu_m_l + 
                         tsh_m_iu_l +
                         amh_ng_m_l +
                         vit_d3_ng_m_l+
                         weight_gain_y_n +
                         hair_growth_y_n +
                         skin_darkening_y_n +
                         hair_loss_y_n +
                         pimples_y_n,
                       data = train.data,
                       family = binomial(link="logit"))


# AIC = 566.45

auc1 <- pROC::roc(train.data$pcos_y_n,
                   fitted(logistic_model1))
tidyr::extract_numeric(auc1$auc)
auc2 <- pROC::roc(train.data$pcos_y_n,
                   fitted(logistic_model2))
tidyr::extract_numeric(auc2$auc)
auc3 <- pROC::roc(train.data$pcos_y_n,
                   fitted(logistic_model3))
tidyr::extract_numeric(auc3$auc)

knitr::kable(data.frame("Model"=c("Physiologic Model", "Clinical Model", "Physiologic and Clinical Model"),
           "AIC"=c(logistic_model1$aic, logistic_model2$aic, logistic_model3$aic), 
           "ROC"=c(tidyr::extract_numeric(auc1$auc), tidyr::extract_numeric(auc2$auc), tidyr::extract_numeric(auc3$auc)
)))
# 
# BIC(logistic_model1)
# BIC(logistic_model2)
# BIC(logistic_model3)


```

Based on these results, we will compare the clinical model to the full clinical and physiological model. While the model including clinical predictors only is more simple both technically (as evidenced by the AIC) and practically (less inputs and no blood testing required), there is an approximately 10% drop in accuracy when excluding the physiological data. We will select these two models and compare performance in applying various machine learning methods below. 

### 4.1 Assessing model classification performance 
* Assessing model performance using ROC curves 

```{r echo=FALSE, fig.height=3, fig.width=5, message=FALSE, warning=FALSE, results = "hide"}

# glimpse(train.data)
# 
# summary(train.data)

train.data.covariates.all <- train.data %>% 
                                 mutate(pcos_y_n = ifelse(pcos_y_n == "1", "Yes", "No")) %>%
                                 drop_na()%>%
  mutate_at(c("pcos_y_n",
              "weight_gain_y_n", 
              "hair_growth_y_n",
              "skin_darkening_y_n",
              "hair_loss_y_n",
              "pimples_y_n"),as.factor) %>%
  mutate_at(c(
    "i_beta_hcg_m_iu_m_l",
    "fsh_m_iu_m_l" ,
    "lh_m_iu_m_l" ,
    "tsh_m_iu_l" ,
    "amh_ng_m_l"), as.numeric) 

glimpse(train.data.covariates.all)

baselinevars_all <- names(dplyr::select(train.data.covariates.all, 
                         !c(pcos_y_n)))
baselinevars_all

model_formula_all <- as.formula(paste("pcos_y_n~ ", 
                               paste(baselinevars_all, 
                                     collapse = "+")))

model_formula_all

train.data.2 <- train.data.covariates.all %>%
                  select(pcos_y_n,
                         i_beta_hcg_m_iu_m_l,
                         fsh_m_iu_m_l,
                         lh_m_iu_m_l,
                         tsh_m_iu_l ,
                         amh_ng_m_l,
                         vit_d3_ng_m_l)%>%mutate(
                           pcos_y_n=as.factor(pcos_y_n)
                         )
  

glimpse(train.data.2)

train.data.covariates.1 <- train.data %>%
  select(
    pcos_y_n,
    i_beta_hcg_m_iu_m_l,
    fsh_m_iu_m_l,
    lh_m_iu_m_l,
    tsh_m_iu_l,
    amh_ng_m_l ,
    vit_d3_ng_m_l,
    weight_gain_y_n ,
    hair_growth_y_n,
    skin_darkening_y_n,
    hair_loss_y_n ,
    pimples_y_n
  ) %>%
  mutate_at(c("pcos_y_n",
              "weight_gain_y_n", 
              "hair_growth_y_n",
              "skin_darkening_y_n",
              "hair_loss_y_n",
              "pimples_y_n"),as.factor) %>%
  mutate_at(c(
    "i_beta_hcg_m_iu_m_l",
    "fsh_m_iu_m_l" ,
    "lh_m_iu_m_l" ,
    "tsh_m_iu_l" ,
    "vit_d3_ng_m_l",
    "amh_ng_m_l"), as.numeric) 

# train.data.2 <- train.data.covariates.1 %>%
#                  drop_na() %>%
#                  mutate(pcos_y_n = ifelse(pcos_y_n == "1", "Yes", "No"))

glimpse(train.data.2)

train.data.3 <- train.data.covariates.1 %>%
                  drop_na() %>%
                  mutate(pcos_y_n = ifelse(pcos_y_n == "1", "Yes", "No"))

# view(train.data.1)

# summary(train.data.1)

baselinevars_3 <- names(dplyr::select(train.data.3, 
                         !c(pcos_y_n)))
baselinevars_3

model_formula_3 <- as.formula(paste("pcos_y_n~ ", 
                               paste(baselinevars_3, 
                                     collapse = "+")))

model_formula_3

## RIDGE cross-validation of our logistic regression models (with binary outcome)
set.seed(504)
ctrl<-trainControl(method = "cv", number = 5,
                   classProbs = TRUE,
                   summaryFunction = twoClassSummary)


## model 1 with all variables
fit.cv.bin.model.all <-train(model_formula_all, trControl = ctrl,
                                 data = train.data.covariates.all, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 0,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")
fit.cv.bin.model.all

# model 1 ROC output (RIDGE): 0.5662222

## model 2 with only hormone variables 
fit.cv.bin.model2 <-train(pcos_y_n ~ ., 
                          trControl = ctrl,
                                 data = train.data.2, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 0,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")
fit.cv.bin.model2

# model 2 ROC output (RIDGE): 0.6045425

## model 3 with hormone variables and clinical symptoms 
fit.cv.bin.model3 <-train(model_formula_3, trControl = ctrl,
                                 data = train.data.3, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 0,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")
fit.cv.bin.model3
# model 3 ROC output (RIDGE): 0.8737665

## LASSO cross-validation of our logistic regression models

set.seed(504)
ctrl<-trainControl(method = "cv", number = 5,
                   classProbs = TRUE,
                   summaryFunction = twoClassSummary)


## model 1 with all variables
fit.cv.bin.model.all.lasso <-train(model_formula_all, trControl = ctrl,
                                 data = train.data.covariates.all, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 1,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")
fit.cv.bin.model.all.lasso

# model 1 output (LASSO): 0.6005752

## model 2 with only hormone variables 
fit.cv.bin.model2.lasso <-train(pcos_y_n ~ ., trControl = ctrl,
                                 data = train.data.2, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 1,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")
fit.cv.bin.model2.lasso

# model 2 output (LASSO): 0.6286078

## model 3 with hormone variables and clinical symptoms 
fit.cv.bin.model3.lasso <-train(model_formula_3, trControl = ctrl,
                                 data = train.data.3, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 1,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")

fit.cv.bin.model3.lasso

# model 3 output (LASSO): 0.8756765

## Cross-validation using ElasticNet

# CV using ElasticNet (for model 1)
set.seed(504)
ctrl<-trainControl(method = "cv", number = 5,
                   classProbs = TRUE,
                   summaryFunction = twoClassSummary)

fit.cv.bin.elastic.model_1 <-train(model_formula_all, trControl = ctrl,
                                   data = train.data.covariates.all, method = "glmnet",
                                   tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  
                                                          lambda = seq(0.05,0.3,by = 0.05)),
                                   verbose = FALSE,
                                   metric="ROC")

fit.cv.bin.elastic.model_1

elastic_model1 <- plot(fit.cv.bin.elastic.model_1)

# CV using ElasticNet (for model 2) 

fit.cv.bin.elastic.model_2 <-train(pcos_y_n ~ ., trControl = ctrl,
                                   data = train.data.2, method = "glmnet",
                                   tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  
                                                          lambda = seq(0.05,0.3,by = 0.05)),
                                   verbose = FALSE,
                                   metric="ROC")

elastic_model2 <- plot(fit.cv.bin.elastic.model_2)

# CV using ElasticNet (for model 3) 
fit.cv.bin.elastic.model_3 <-train(model_formula_3, trControl = ctrl,
                                   data = train.data.3, method = "glmnet",
                                   tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  
                                                          lambda = seq(0.05,0.3,by = 0.05)),
                                   verbose = FALSE,
                                   metric="ROC")

fit.cv.bin.elastic.model_3

# elastic_model3 <- plot(fit.cv.bin.elastic.model_3, title = "Cross Validation with ElasticNet for Model 3")

# cowplot::plot_grid(elastic_model2, elastic_model3)

```

#### Figure 1. ROC curves: clinical model (blue) vs. clinical and physiologic model (green)

```{r echo=FALSE, fig.height=4, fig.width=4, message=FALSE, warning=FALSE}
## plotting ROC and AUC 

# plot(pROC::roc(train.data$pcos_y_n,
#                    fitted(logistic_model1)),
#                col = "red", 
#                main = "ROC curves: model 1 (red) vs. model 2 (blue) \n vs. model 3 (green)")
plot(pROC::roc(train.data$pcos_y_n,
                   fitted(logistic_model2)),
               print.auc = T, 
               col = "blue",
                print.auc.y = .6)
plot(pROC::roc(train.data$pcos_y_n,
                   fitted(logistic_model3)),
               print.auc = T, 
               col = "green", 
               add = T,
     print.auc.y=.4)

```

```{r echo=FALSE, message=FALSE, warning = FALSE, results = "hide"}
# library(bestglm)

#train.data.covariates.1 <- train.data %>%
#  select(
#     pcos_y_n,
#     i_beta_hcg_m_iu_m_l,
#     fsh_m_iu_m_l,
#     lh_m_iu_m_l,
#     tsh_m_iu_l,
#     amh_ng_m_l ,
#     weight_gain_y_n ,
#     hair_growth_y_n,
#     skin_darkening_y_n,
#     hair_loss_y_n ,
#     pimples_y_n
#   ) %>%
#   mutate_at(c("pcos_y_n",
#               "weight_gain_y_n", 
#               "hair_growth_y_n",
#               "skin_darkening_y_n",
#               "hair_loss_y_n",
#               "pimples_y_n"),as.factor) %>%
#   mutate_at(c(
#     "i_beta_hcg_m_iu_m_l",
#     "fsh_m_iu_m_l" ,
#     "lh_m_iu_m_l" ,
#     "tsh_m_iu_l" ,
#     "amh_ng_m_l"), as.numeric)
# 
# x_tr <- train.data.covariates.1 
# # x_tr$pcos_labelled <- ifelse(x_tr$pcos_labelled=="yes",1,0)
# colnames(x_tr)[1] <- "y"
# res.bestglm <-bestglm::bestglm(Xy = x_tr,
#             family = binomial,
#             IC = "AIC",                 # Information criteria for
#             method = "exhaustive")
# 
# ## Show top 5 models
# res.bestglm$BestModels
# 
# summary(res.bestglm$BestModel)

```

### 4.2 Modelling our data using Trees and Forests 

We next pursued a Random Forest approach for our selection of variables, using both the Clinical and Clinical + Physiological Parameters for our Random Forest Modelling. Random Forest modelling was also used to examine variable importance. 

```{r echo=FALSE, message=FALSE, warning = FALSE, results = "hide"}

train.data.covariates.model2 <- train.data %>%
  select(
    pcos_y_n,
    weight_gain_y_n ,
    hair_growth_y_n,
    skin_darkening_y_n,
    hair_loss_y_n ,
    pimples_y_n
  ) %>% mutate(
  pcos_labelled = as.factor(ifelse(pcos_y_n==1, "yes", "no")))

train.data.covariates.model2

train.data.covariates.model3 <- train.data %>%
  select(
    pcos_y_n,
    i_beta_hcg_m_iu_m_l,
    fsh_m_iu_m_l ,
    lh_m_iu_m_l ,
    tsh_m_iu_l ,
    amh_ng_m_l ,
    vit_d3_ng_m_l,
    weight_gain_y_n ,
    hair_growth_y_n,
    skin_darkening_y_n,
    hair_loss_y_n ,
    pimples_y_n
  ) %>% mutate(
  amh_ng_m_l = as.numeric(amh_ng_m_l),
  pcos_labelled = as.factor(ifelse(pcos_y_n==1, "yes", "no")))
```


Random Forest 

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)#Can also fit with caret
set.seed(504)

caret_rf_model2 <- train(
  pcos_labelled ~ .,
  data = select(train.data.covariates.model2, -pcos_y_n),                    
  method = "ranger",
  metric = "ROC",
  na.action = na.pass,
  trControl = trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary),
  importance="impurity")

caret_rf_model3 <- train(
  pcos_labelled ~ .,
  data = select(filter(train.data.covariates.model3,!is.na(amh_ng_m_l)), -pcos_y_n),                    
  method = "ranger",
  metric = "ROC",
  na.action = na.pass,
  trControl = trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary),
  importance="impurity"
)

knitr::kable(data.frame(
  "Model" = c("Clinical and Physiologic Model", 
              "Clinical Model"),
  "ROC"=c(mean(caret_rf_model3$resample$ROC),mean(caret_rf_model2$resample$ROC)), 
  "Sensitivity" = c(mean(caret_rf_model3$resample$Sens), mean(caret_rf_model2$resample$Sens)),
  "Specificity" = c(mean(caret_rf_model3$resample$Spec), mean(caret_rf_model2$resample$Spec)
           )))

```

<!-- # XGboost -->

```{r echo=FALSE, message=FALSE, warning = FALSE, results = "hide"}
set.seed(123)
xgb_grid_1  <-  expand.grid(
                  nrounds = 50,
                  eta = c(0.03),
                  max_depth = 1,
                  gamma = 0,
                  colsample_bytree = 0.6,
                  min_child_weight = 1,
                  subsample = 0.5
                )

caret_xgb_model2 <- caret::train(pcos_labelled ~., data = select(train.data.covariates.model2, -(pcos_y_n)),
                         method = "xgbTree",
                         metric = "ROC",
                         tuneGrid=xgb_grid_1,
                         na.action = na.pass,
                         trControl = trainControl(method = "cv", number = 5,classProbs = T, summaryFunction = twoClassSummary))

caret_xgb_model3 <- caret::train(pcos_labelled ~., data = select(train.data.covariates.model3, -(pcos_y_n)),
                         method = "xgbTree",
                         metric = "ROC",
                         tuneGrid=xgb_grid_1,
                         na.action = na.pass,
                         trControl = trainControl(method = "cv", number = 5,classProbs = T, summaryFunction = twoClassSummary))


```

<!-- Comparison code for later  -->

<!-- Data Wrangling for the test data  -->

```{r echo=FALSE, message=FALSE, warning = FALSE, results = "hide"}

test.data.lasso <- test.data %>%
  select(
    pcos_y_n,
    i_beta_hcg_m_iu_m_l,
    fsh_m_iu_m_l,
    lh_m_iu_m_l,
    tsh_m_iu_l,
    amh_ng_m_l ,
    vit_d3_ng_m_l,
    weight_gain_y_n ,
    hair_growth_y_n,
    skin_darkening_y_n,
    hair_loss_y_n ,
    pimples_y_n
  ) %>%
  mutate_at(c("pcos_y_n",
              "weight_gain_y_n", 
              "hair_growth_y_n",
              "skin_darkening_y_n",
              "hair_loss_y_n",
              "pimples_y_n"),as.factor) %>%
  mutate_at(c(
    "i_beta_hcg_m_iu_m_l",
    "fsh_m_iu_m_l" ,
    "lh_m_iu_m_l" ,
    "tsh_m_iu_l" ,
    "vit_d3_ng_m_l",
    "amh_ng_m_l"), as.numeric) %>%
  mutate(pcos_labelled=as.factor(ifelse(pcos_y_n==1, "yes", "no")))%>%
  select(!pcos_labelled)


test.data <- test.data %>%
  select(
    pcos_y_n,
    i_beta_hcg_m_iu_m_l,
    fsh_m_iu_m_l,
    lh_m_iu_m_l,
    tsh_m_iu_l,
    amh_ng_m_l ,
    vit_d3_ng_m_l, 
    weight_gain_y_n ,
    hair_growth_y_n,
    skin_darkening_y_n,
    hair_loss_y_n ,
    pimples_y_n
  ) %>%
  mutate_at(c("pcos_y_n",
              "weight_gain_y_n", 
              "hair_growth_y_n",
              "skin_darkening_y_n",
              "hair_loss_y_n",
              "pimples_y_n"),as.factor) %>%
  mutate_at(c(
    "i_beta_hcg_m_iu_m_l",
    "vit_d3_ng_m_l", 
    "fsh_m_iu_m_l" ,
    "lh_m_iu_m_l" ,
    "tsh_m_iu_l" ,
    "amh_ng_m_l"), as.numeric) %>%
  mutate(pcos_labelled=as.factor(ifelse(pcos_y_n==1, "yes", "no")))%>%
  select(!pcos_y_n)



```


### Figure X. Comparative performance on the training data

```{r echo=FALSE, include=TRUE}
model_list <- list(RF.Clinical.Physiological = caret_rf_model3,
                   XG.Clinical.Physiological = caret_xgb_model3, 
                   RF.Clinical = caret_rf_model2,
                   XG.Clinical = caret_xgb_model2
                   # , Rpart_DT = caret_tree, Bagging = caret_bag
                   )
resamples <- resamples(model_list)

#box plot
bwplot(resamples, metric="ROC")
```
```{r echo=FALSE, include=FALSE, results='hide'}
dotplot(resamples, metric="ROC")

```
<!-- ### Evaluating on the test set  -->

<!-- #### Regularized Logistic Regression  -->

<!-- GLM Net with LASSO - Clinical Model -->

```{r echo=FALSE, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
# prediction on Test data set
pred_rf <- stats::predict(fit.cv.bin.model2.lasso, test.data, type="prob")
# Confusion Matrix 
pred_rf$prediction <- ifelse(pred_rf$Yes < 0.5, "No", "Yes") %>% factor()
tab <- table(pred_rf$prediction, test.data$pcos_labelled)
tn <- tab[1] # TN
fn <- tab[2] # FN
fp <- tab[3] # FP
tp <- tab[4] # TP
accuracy = (tp + tn) / (tp + tn + fp + fn)
accuracy
sensitivity = tp/(tp+fn)
specificity = tn /(fp+tn)
clinical.lasso.row<-data.frame("Model"=
             "LASSO: Clinical Parameters", 
           "Accuracy"=accuracy, 
           "Sensitivity"=sensitivity, 
           "Specificity"=specificity)
```

<!-- GLM Net with LASSO - Clinical and Physiologic Model -->

```{r echo=FALSE, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
# prediction on Test data set
pred_rf <- stats::predict(fit.cv.bin.model3.lasso, test.data, type="prob")
# Confusion Matrix 
pred_rf$prediction <- ifelse(pred_rf$Yes < 0.5, "No", "Yes") %>% factor()
tab <- table(pred_rf$prediction, test.data$pcos_labelled)
tn <- tab[1] # TN
fn <- tab[2] # FN
fp <- tab[3] # FP
tp <- tab[4] # TP
accuracy = (tp + tn) / (tp + tn + fp + fn)
accuracy
sensitivity = tp/(tp+fn)
specificity = tn /(fp+tn)
clinical.physiologic.lasso.row<-data.frame("Model"=
             "LASSO: Clinical and Physiologic Parameters", 
           "Accuracy"=accuracy, 
           "Sensitivity"=sensitivity, 
           "Specificity"=specificity)
```


<!-- #### XGBoost -->

<!-- XGBoost - Clinical Model  -->

```{r echo=FALSE, message=FALSE, warning=FALSE, include=TRUE, results='hide'}

# prediction on Test data set
pred_rf <- predict(caret_xgb_model2, test.data)
# Confusion Matrix 
xgboost.clinical <- confusionMatrix(pred_rf, test.data$pcos_labelled, positive="yes")
# Prediction Probabilities
pred_prob_rf <- predict(caret_xgb_model2, test.data, type="prob")
# ROC value
roc_rf <- pROC::roc(test.data$pcos_labelled, pred_prob_rf$yes)
# Confusion Matrix for Random Forest Model
xgboost.clinical
```

<!-- XGBoost - Clinical and Physiologic Model  -->

```{r echo=FALSE, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
# prediction on Test data set
pred_rf <- predict(caret_xgb_model3, test.data)
# Confusion Matrix 
xgboost.clinical.physiologic <- confusionMatrix(pred_rf, test.data$pcos_labelled, positive="yes")
# Prediction Probabilities
pred_prob_rf <- predict(caret_xgb_model3, test.data, type="prob")
# ROC value
roc_rf <- pROC::roc(test.data$pcos_labelled, pred_prob_rf$yes)
# Confusion Matrix for Random Forest Model
xgboost.clinical.physiologic
```

<!-- #### Random Forest  -->

<!-- Random Forest - Clinical Model  -->

```{r echo=FALSE, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
# prediction on Test data set
pred_rf <- predict(caret_rf_model2, test.data)
# Confusion Matrix 
randomForest.clinical <- confusionMatrix(pred_rf, test.data$pcos_labelled, positive="yes")
# Prediction Probabilities
pred_prob_rf <- predict(caret_rf_model2, test.data, type="prob")
# ROC value
roc_rf <- pROC::roc(test.data$pcos_labelled, pred_prob_rf$yes)
# Confusion Matrix for Random Forest Model
randomForest.clinical
```


<!-- Random Forest - Clinical and Physiologic Model  -->

```{r echo=FALSE, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
# prediction on Test data set
pred_rf <- predict(caret_rf_model3, test.data)
# Confusion Matrix 
randomForest.clinical.physiologic <- confusionMatrix(pred_rf, test.data$pcos_labelled, positive="yes")
# Prediction Probabilities
pred_prob_rf <- predict(caret_rf_model3, test.data, type="prob")
# ROC value
roc_rf <- pROC::roc(test.data$pcos_labelled, pred_prob_rf$yes)
# Confusion Matrix for Random Forest Model
randomForest.clinical.physiologic
```

### Summarised performance

#### Table 2. Comparative performance of two models across three different modelling methodologies

```{r echo=FALSE, message=FALSE, warning = FALSE}


knitr::kable(
  rbind(data.frame(
  "Model"=c("Clinical Parameters: Random Forest", 
            "Clinical and Physiologic Parameters: Random Forest", 
            "Clinical Parameters: XGBoost", 
            "Clinical and Physiologic Parameters: XGBoost" 
            ), 
  "Accuracy"=c(
    randomForest.clinical$overall[1], 
    randomForest.clinical.physiologic$overall[1], 
    xgboost.clinical$overall[1], 
    xgboost.clinical.physiologic$overall[1]
  ),
  "Sensitivity"=c(
    randomForest.clinical$byClass[1], 
    randomForest.clinical.physiologic$byClass[1], 
    xgboost.clinical$byClass[1], 
    xgboost.clinical.physiologic$byClass[1]
  ), 
  "Specificity"=c(
    randomForest.clinical$byClass[2], 
    randomForest.clinical.physiologic$byClass[2], 
    xgboost.clinical$byClass[2], 
    xgboost.clinical.physiologic$byClass[2]
  )), 
  clinical.lasso.row,
  clinical.physiologic.lasso.row))
# randomForest.clinical$overall[1]

```

## 5. Discussion and Conclusion
### 5.1 Discussion

We found the best ML model which balances accuracy, specificity and clinical utility to be the Random Forest model with clinical parameters (n=5 clinical symptoms: weight gain, hair growth + skin darkening, hair loss and pimples). This model performed with the following accuracy = 88.1%, sensitivity = 71.2% and specificity = 96.3% when applied to our test dataset. Not only did this model have the highest accuracy of the six models we compared, the model also performed well on sensitivity and specificity. We opted to use a Random Forest approach as part of trying different ML models as a recent study in the literature indicating Random Forest models perform the most accurately on a wide variety of datasets^6^. One of the drawbacks of the Random Forest is its black box nature, making it harder to determine and/or interpret how the model is making its predictions. This could pose a challenge to clinicians trying to troubleshoot our improve upon the model. Given our use of physiological parameters such as the hormones human chorionic gonadotropin (HCG) and anti-mullerian hormone (AMH), which may be used as proxies to determine pregnancy status and fertility respectively, it is important that privacy is maintained and also physicians have access to the model's de-anonymized input data.   

### 5.2 Conclusions

PCOS status can be predicted with a reasonably high level of accuracy (88.1% with our model) using a combination of clinical and physiological parameters that are easily measurable in a general clinical practice setting. By optimizing for specificity, patients likely to be PCOS positive can be selected for more expensive and invasive testing, thus reducing the cost of care and improving patient experience for the population of at-risk women. Implementation of model-based screening should be considered particularly in resource-limited settings, where access to advanced diagnostics is challenging due to cost or geography. 

## References 
1. Teede, H., Misso, M., Tassone, E. C., Dewailly, D., Ng, E. H., Azziz, R., ... & Norman, R. J. (2018). Recommendations from the international evidence-based guideline for the assessment and management of polycystic ovary syndrome. Human Reproduction, 33(9), 1602-1618. https://doi.org/10.1093/humrep/dey256
2. Bozdag, G., Mumusoglu, S., Zengin, D., & Karabulut, E. (2016). The prevalence and phenotypic features of polycystic ovary syndrome: a systematic review and meta-analysis. Human Reproduction, 31(12), 28412855. https://doi.org/10.1093/humrep/dew218
3. R for Data Science by Hadley Wickham (https://r4ds.had.co.nz)
4. Ajmal N, Khan SZ, Shaikh R. Polycystic ovary syndrome (PCOS) and genetic predisposition: A review article. Eur J Obstet Gynecol Reprod Biol X. 2019 Jun 8;3:100060. doi: 10.1016/j.eurox.2019.100060. PMID: 31403134; PMCID: PMC6687436.
5. Hart R, Doherty DA. The potential implications of a PCOS diagnosis on a woman's long-term health using data linkage. J Clin Endocrinol Metab. 2015 Mar;100(3):911-9. doi: 10.1210/jc.2014-3886. Epub 2014 Dec 22. Erratum in: J Clin Endocrinol Metab. 2015 Jun;100(6):2502. PMID: 25532045.
6. Fernndez-Delgado, M., Cernadas E., Barro S., Amorim D. Do we need hundreds of classifiers to solve real world classification problems? The Journal of Machine Learning Research. Volume 15. Issue 1, pp 31333181. 
7. Deswal R, Narwal V, Dang A, Pundir CS. The Prevalence of Polycystic Ovary Syndrome: A Brief Systematic Review. J Hum Reprod Sci. 2020 Oct-Dec;13(4):261-271. doi: 10.4103/jhrs.JHRS_95_18. Epub 2020 Dec 28. PMID: 33627974; PMCID: PMC7879843.

## Appendix 

* Appendix Plot 1, as part of EDA, showing missing values in our dataset
```{r EDA1, echo=FALSE, fig.align = "center", fig.height=10, fig.width=10, message=FALSE, warning=FALSE, results="hide"}
# checking for missing values in dataset 
plot_missing(PCOS_data_without_infertility_formatted, title = "Plot showing missing values")

# removing the missing values from the data

PCOS_data_without_infertility_formatted <- PCOS_data_without_infertility_formatted %>%
                                              drop_na()

# view(PCOS_data_without_infertility_formatted)

```

* Appendix plot 2, as part of EDA, examining correlations between continuous features 

```{r EDA3, echo=FALSE, fig.height=10, fig.width=20, message=FALSE, warning=FALSE}
# performing a correlation analysis for continuous features 
plot_correlation(PCOS_data_without_infertility_formatted, title = "Bivariate analysis to visualize our covariation between our continuous variables", theme_config = list("plot.title" = element_text(size = 24)), type = 'continuous')
```

* From this correlation plot, we find that several continuous variables do co-vary with one another.
* Specifically, as we would expect, we find a positive correlation between the variables waist and hip (in inches) with weight. We find the same positive correlation for BMI. 
* Another obvious correlation we observe is that between age (in years) and marriage (in years) 
* Appendix Plot 3 showing boxplots generated to investigate potential associations between continuous variables and PCOS status. 

### Appendix Plot 3 - Univariable distributions for continuous variables  

```{r boxplots, echo=FALSE, fig.height=10, fig.width=20, message=FALSE, warning=FALSE, results= "hide"}

plot_boxplot(PCOS_data_without_infertility_formatted, by = "pcos_y_n", title = "Boxplots for continous variables by PCOS status")



```

```{r EDA2, eval=FALSE, fig.height=10, fig.width=20, message=FALSE, warning=FALSE, include=FALSE}
# plotting a histogram for each continuous feature 
plot_histogram(PCOS_data_without_infertility_formatted, theme_config = list("plot.title" = element_text(size = 24)), title = "Histograms for each continuous variable - to visualize distributions", )

```

### Appendix Plot 4 - Penalized logistic regression with cross-validation (RIDGE and LASSO)

```{r echo=FALSE, fig.height=3, fig.width=5, message=FALSE, warning=FALSE, results = "hide"}

# glimpse(train.data)
# 
# summary(train.data)

train.data.covariates.all <- train.data %>% 
                                 mutate(pcos_y_n = ifelse(pcos_y_n == "1", "Yes", "No")) %>%
                                 drop_na()%>%
  mutate_at(c("pcos_y_n",
              "weight_gain_y_n", 
              "hair_growth_y_n",
              "skin_darkening_y_n",
              "hair_loss_y_n",
              "pimples_y_n"),as.factor) %>%
  mutate_at(c(
    "i_beta_hcg_m_iu_m_l",
    "fsh_m_iu_m_l" ,
    "lh_m_iu_m_l" ,
    "tsh_m_iu_l" ,
    "amh_ng_m_l"), as.numeric) 

glimpse(train.data.covariates.all)

baselinevars_all <- names(dplyr::select(train.data.covariates.all, 
                         !c(pcos_y_n)))
baselinevars_all

model_formula_all <- as.formula(paste("pcos_y_n~ ", 
                               paste(baselinevars_all, 
                                     collapse = "+")))

model_formula_all

train.data.2 <- train.data.covariates.all %>%
                  select(pcos_y_n,
                         i_beta_hcg_m_iu_m_l,
                         fsh_m_iu_m_l,
                         lh_m_iu_m_l,
                         tsh_m_iu_l ,
                         amh_ng_m_l)%>%mutate(
                           pcos_y_n=as.factor(pcos_y_n)
                         )
  

glimpse(train.data.2)

train.data.covariates.1 <- train.data %>%
  select(
    pcos_y_n,
    i_beta_hcg_m_iu_m_l,
    fsh_m_iu_m_l,
    lh_m_iu_m_l,
    tsh_m_iu_l,
    amh_ng_m_l ,
    weight_gain_y_n ,
    hair_growth_y_n,
    skin_darkening_y_n,
    hair_loss_y_n ,
    pimples_y_n
  ) %>%
  mutate_at(c("pcos_y_n",
              "weight_gain_y_n", 
              "hair_growth_y_n",
              "skin_darkening_y_n",
              "hair_loss_y_n",
              "pimples_y_n"),as.factor) %>%
  mutate_at(c(
    "i_beta_hcg_m_iu_m_l",
    "fsh_m_iu_m_l" ,
    "lh_m_iu_m_l" ,
    "tsh_m_iu_l" ,
    "amh_ng_m_l"), as.numeric) 

# train.data.2 <- train.data.covariates.1 %>%
#                  drop_na() %>%
#                  mutate(pcos_y_n = ifelse(pcos_y_n == "1", "Yes", "No"))

glimpse(train.data.2)

train.data.3 <- train.data.covariates.1 %>%
                  drop_na() %>%
                  mutate(pcos_y_n = ifelse(pcos_y_n == "1", "Yes", "No"))

# view(train.data.1)

# summary(train.data.1)

baselinevars_3 <- names(dplyr::select(train.data.3, 
                         !c(pcos_y_n)))
baselinevars_3

model_formula_3 <- as.formula(paste("pcos_y_n~ ", 
                               paste(baselinevars_3, 
                                     collapse = "+")))

model_formula_3

## RIDGE cross-validation of our logistic regression models (with binary outcome)
set.seed(504)
ctrl<-trainControl(method = "cv", number = 5,
                   classProbs = TRUE,
                   summaryFunction = twoClassSummary)


## model 1 with all variables
fit.cv.bin.model.all <-train(model_formula_all, trControl = ctrl,
                                 data = train.data.covariates.all, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 0,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")
fit.cv.bin.model.all

# model 1 ROC output (RIDGE): 0.5662222

## model 2 with only hormone variables 
fit.cv.bin.model2 <-train(pcos_y_n ~ ., trControl = ctrl,
                                 data = train.data.2, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 0,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")
fit.cv.bin.model2

# model 2 ROC output (RIDGE): 0.6045425

## model 3 with hormone variables and clinical symptoms 
fit.cv.bin.model3 <-train(model_formula_3, trControl = ctrl,
                                 data = train.data.3, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 0,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")
fit.cv.bin.model3
# model 3 ROC output (RIDGE): 0.8737665

## LASSO cross-validation of our logistic regression models

set.seed(504)
ctrl<-trainControl(method = "cv", number = 5,
                   classProbs = TRUE,
                   summaryFunction = twoClassSummary)


## model 1 with all variables
fit.cv.bin.model.all.lasso <-train(model_formula_all, trControl = ctrl,
                                 data = train.data.covariates.all, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 1,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")
fit.cv.bin.model.all.lasso

# model 1 output (LASSO): 0.6005752

## model 2 with only hormone variables 
fit.cv.bin.model2.lasso <-train(pcos_y_n ~ ., trControl = ctrl,
                                 data = train.data.2, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 1,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")
fit.cv.bin.model2.lasso

# model 2 output (LASSO): 0.6286078

## model 3 with hormone variables and clinical symptoms 
fit.cv.bin.model3.lasso <-train(model_formula_3, trControl = ctrl,
                                 data = train.data.3, method = "glmnet",
                                 lambda= 0,
                                 tuneGrid = expand.grid(alpha = 1,  
                                                        lambda = 0),
                                 verbose = FALSE,
                                 metric="ROC")

fit.cv.bin.model3.lasso

# model 3 output (LASSO): 0.8756765

## Cross-validation using ElasticNet

# CV using ElasticNet (for model 1)
set.seed(504)
ctrl<-trainControl(method = "cv", number = 5,
                   classProbs = TRUE,
                   summaryFunction = twoClassSummary)

fit.cv.bin.elastic.model_1 <-train(model_formula_all, trControl = ctrl,
                                   data = train.data.covariates.all, method = "glmnet",
                                   tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  
                                                          lambda = seq(0.05,0.3,by = 0.05)),
                                   verbose = FALSE,
                                   metric="ROC")

fit.cv.bin.elastic.model_1

elastic_model1 <- plot(fit.cv.bin.elastic.model_1)

# CV using ElasticNet (for model 2) 

fit.cv.bin.elastic.model_2 <-train(pcos_y_n ~ ., trControl = ctrl,
                                   data = train.data.2, method = "glmnet",
                                   tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  
                                                          lambda = seq(0.05,0.3,by = 0.05)),
                                   verbose = FALSE,
                                   metric="ROC")

elastic_model2 <- plot(fit.cv.bin.elastic.model_2)

# CV using ElasticNet (for model 3) 
fit.cv.bin.elastic.model_3 <-train(model_formula_3, trControl = ctrl,
                                   data = train.data.3, method = "glmnet",
                                   tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  
                                                          lambda = seq(0.05,0.3,by = 0.05)),
                                   verbose = FALSE,
                                   metric="ROC")

fit.cv.bin.elastic.model_3

elastic_model3 <- plot(fit.cv.bin.elastic.model_3, title = "Cross Validation with ElasticNet for Model 3")

cowplot::plot_grid(elastic_model2, elastic_model3)

```

### Appendix Section 5 - Model cross-validation (without RIDGE or LASSO) (using the caret package)
* We performed model cross-validation on our three logistic regression models and generated a summary of the cross-validation results. 

```{r echo=FALSE, message=FALSE, warning=FALSE, results = "hide"}
## cross-validation 

cv_of_data <- caret::trainControl(
  method = "repeatedcv", 
  number = 5, #five fold cross validation 
  repeats = 3 # repeat 3 times
)

set.seed(504)
# cv_of_model1 <- caret::train(
#                               pcos_y_n ~ 
#                        , 
#                               data = train.data, 
#                               method = "glm",
#                               family = "binomial",
#                               trControl = cv_of_data
#                             )



cv_of_model2 <- caret::train( pcos_y_n ~ 
                               weight_gain_y_n +
                              hair_growth_y_n +
                              skin_darkening_y_n +
                         hair_loss_y_n +
                         pimples_y_n, 
                              data = train.data, 
                              method = "glm",
                              family = "binomial",
                              trControl = cv_of_data
                            )

cv_of_model3 <- caret::train(
                              pcos_y_n ~ i_beta_hcg_m_iu_m_l + fsh_m_iu_m_l + lh_m_iu_m_l + tsh_m_iu_l + amh_ng_m_l +
                              weight_gain_y_n + hair_growth_y_n + skin_darkening_y_n + hair_loss_y_n + pimples_y_n, 
                              data = train.data, 
                              method = "glm",
                              family = "binomial",
                              trControl = cv_of_data
                            )

summary(
  resamples(
    list(
      # model1 = cv_of_model1, 
      model2 = cv_of_model2, 
      model3 = cv_of_model3
    )
  )
)$statistics$Accuracy

### Cross-validation RIDGE 

set.seed(504)
cv_of_data <- caret::trainControl(
  method = "repeatedcv", 
  number = 5, #five fold cross validation 
  repeats = 3 # repeat 3 times
)


```

### Appendix Plot 6 - Variable importance (based on Random Forest model)
```{r echo=FALSE, message=FALSE, warning = FALSE, results = "hide"}
vip::vip(caret_rf_model3)+theme_bw()
```